import urllib
from seo.models import SeoSite


def build_seo_redirects():
    """
    Takes the output file generated by get_jpi_file and uses it to create 
    SeoSiteRedirect Objects. This is a utility function and has no input/return.
    
    """
    from seo.models import SeoSiteRedirect
    from wildcard import site_list
    for site in site_list.site_dict:
        site_domain = site_list.site_dict[site][0]
        seo_site = SeoSite.objects.get(domain=site_domain)
        redirects = SeoSiteRedirect.objects.filter(seosite=seo_site)
        site_redirect = SeoSiteRedirect(redirect_url=site, seosite=seo_site)
        try:
            site_redirect.save()
            print "Created: %s" % site_redirect
        except:
            print "Already Exists: %s" % site_redirect


def get_jpi_file(count=10, filename="site_list.py"):
    """
    Query the .jobs Universe API to generate a static dictionary file that
    contains matches to the defined SEO sites. This needs to be an external file
    so that (1) We don't get fale matches from disambiguation when stripping
    invalid subdomains, and (2) so we can shut down the API.

    This method is intended to be run from the command line.
    
    Inputs:
    :count:         how many results to return. Set to False to get everything.
    :filename:      the filename to use when writing the file
    
    Returns:
    :complete_msg:  Message stating the file was written.
    
    Writes:
    :site_list.py:  Python file with a single dict object defined.
    
    """
    if count:     
        sites = SeoSite.objects.filter(business_units__isnull=True)[0:count]
    else:
        sites = SeoSite.objects.filter(business_units__isnull=True)
        
    site_list = []

    for site in sites:
        site_tuple = jpi_reader(site.domain, "results")
        
        if site_tuple:
            site_list.append(site_tuple)
            if ".metro." in site_tuple[1]:
                site_tuple_check_metro = (site_tuple[0], 
                                          site_tuple[1].replace(".metro.", "."),
                                          site_tuple[2])
                site_list.append(site_tuple_check_metro)

    site_list = sorted(site_list, key=lambda source: source[2])
    site_dict = {}
    record_count = 0
    
    for site in site_list:
        key_site = site[1].replace("/", "")
        profile_domain = site[0]
        # canonical url or disambiguated
        match_type = site[2]
        site_dict[key_site] = (profile_domain, match_type)
        record_count += 1

    # don't bother with I/O if there aren't any records
    if record_count > 0:
        FILE = open(filename, "w")
        FILE.write("site_dict = %s" % site_dict)
        FILE.close()
        complete_msg = "%s created. (%s records)" % (filename, record_count)
    else:
        complete_msg = "No file written. Something went wrong."
        
    return complete_msg
    

def jpi_reader(domain, lookup_type="results"):
    """
    Makes a call to the JPI framework datatype. The result is an XML file.
    
    Inputs:
    :domain:        the url to lookup. 
    :lookup_type:   the type of domain to retrieve. The JPI returns both
                    results urls and profile urls. This defaults to the results 
                    url.    
    Returns:
    :site_tuple:    a tuple result => (domain, retrieved url, xml source).
    
    """
    node_dict = {"results": "cu", "profile": "pd"}
    jpi_call = "//d2e48ltfsb5exy.cloudfront.net/?query=%s&data=framework&redirect=0" % domain
    url_object = urllib.urlopen(jpi_call)
    url_string = url_object.read()
    tag_result = get_jpi_tag_value(node_dict[lookup_type], url_string)
    if len(tag_result) > 0:
        site_tuple = (domain, tag_result[0], tag_result[1])
    else:
        site_tuple = False
    return site_tuple


def get_jpi_tag_value(needle, haystack):
    """
    Retrieves the value of a specified xml node. The assumption is made that
    the node is unique to the xml file regardless of nesting. If it is not
    unique, this will return the value of the first node. If the tag is not
    found at all, it will check the disambiguated url node. If that also is
    not found, it will return an empty set.
    
    Inputs:
    :needle:    the tag name to look for, without brackets
    :haystack:  the string to search in. Should be an xml string.
    
    Returns:
    :tag_value: a list object containing the contents of the tag and the source 
                description 
    
    """
    tag = "<%s>" % needle
    endtag = "</%s>" % needle
    backup_string = "<![CDATA[Disambiguated URL: "
    
    if tag in haystack:
        string_start = haystack.find(tag)+len(tag)
        string_end = haystack.find(endtag)
        tag_value = [haystack[string_start:string_end], 'canonical url']
    elif backup_string in haystack:        
        string_start = haystack.find(backup_string)+len(backup_string)
        haystack = haystack[string_start:-1]
        string_end = haystack.find("]]")
        tag_value = [haystack[0:string_end], 'disambiguated']
    else:
        tag_value = []
    
    return tag_value
