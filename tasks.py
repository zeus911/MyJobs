import boto
from datetime import date, timedelta, datetime
from itertools import chain, izip_longest
import logging
import os
import pysolr
import sys
import traceback
from urllib2 import HTTPError, URLError
import urlparse
import uuid

from celery import group
from celery.task import task

from django.conf import settings
from django.contrib.sitemaps import ping_google
from django.contrib.contenttypes.models import ContentType
from django.core import mail
from django.core.urlresolvers import reverse_lazy
from django.template.loader import render_to_string
from django.db.models import Q

from seo.models import Company, SeoSite
from myjobs.models import EmailLog, User, STOP_SENDING, BAD_EMAIL
from mymessages.models import Message
from mysearches.models import SavedSearch, SavedSearchDigest
from mypartners.models import PartnerLibrary
from mypartners.helpers import get_library_partners
import import_jobs
from postajob.models import Job
from registration.models import ActivationProfile
from solr import helpers
from solr.models import Update
from solr.signals import object_to_dict, profileunits_to_dict

logger = logging.getLogger(__name__)

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = "/de/data/"
sys.path.insert(0, os.path.join(BASE_DIR))
sys.path.insert(0, os.path.join(BASE_DIR, '../'))
os.environ['DJANGO_SETTINGS_MODULE'] = 'settings'
FEED_FILE_PREFIX = "dseo_feed_"


@task(name='tasks.send_search_digest', ignore_result=True,
      default_retry_delay=180, max_retries=2, bind=True)
def send_search_digest(self, search):
    """
    Task used by send_send_search_digests to send individual digest or search
    emails.

    Inputs:
    :search: SavedSearch or SavedSearchDigest instance to be mailed
    """
    try:
        search.send_email()
    except (ValueError, URLError, HTTPError) as e:
        if self.request.retries < 2:  # retry sending email twice
            raise send_search_digest.retry(arg=[search], exc=e)
        else:
            # After the initial try and two retries, disable the offending
            # saved search
            search.disable_or_fix()


@task(name='tasks.update_partner_library', ignore_result=True,
      default_retry_delay=180, max_retries=2)
def update_partner_library(path=None, quiet=False):
    added = 0
    skipped = 0
    if not quiet:
        if not path:
            print "Connecting to OFCCP Directory...."

        print "Parsing data for PartnerLibrary information..."

    for partner in get_library_partners(path):
        # the second join + split take care of extra internal whitespace
        fullname = " ".join(" ".join([partner.first_name,
                                      partner.middle_name,
                                      partner.last_name]).split())
        emails = [email.strip() for email in partner.email_id.split(';', 1)]

        for email in emails:
            if not PartnerLibrary.objects.filter(contact_name=fullname,
                                                 st=partner.st,
                                                 city=partner.city,
                                                 email=email):
                PartnerLibrary.objects.create(
                    name=partner.organization_name,
                    uri=partner.website,
                    region=partner.region,
                    state=partner.state,
                    area=partner.area,
                    contact_name=fullname,
                    phone=partner.phone,
                    phone_ext=partner.phone_ext,
                    alt_phone=partner.alt_phone,
                    fax=partner.fax,
                    email=email,
                    street1=partner.street1,
                    street2=partner.street2,
                    city=partner.city,
                    st=partner.st,
                    zip_code=partner.zip_code,
                    is_minority=partner.minority,
                    is_female=partner.female,
                    is_disabled=partner.disabled,
                    is_disabled_veteran=partner.disabled_veteran,
                    is_veteran=partner.veteran)
                added += 1
            else:
                skipped += 1
    if not quiet:
        print "%d records added and %d records skipped." % (added, skipped)


@task(name='tasks.send_search_digests', ignore_result=True)
def send_search_digests():
    """
    Daily task to send saved searches. If user opted in for a digest, they
    receive it daily and do not get individual saved search emails. Otherwise,
    each active saved search is sent individually.
    """

    def filter_by_time(qs):
        """
        Filters the provided query set for emails that should be sent today

        Inputs:
        :qs: query set to be filtered

        Outputs:
        :qs: filtered query set containing today's outgoing emails
        """
        today = datetime.today()
        day_of_week = today.isoweekday()

        daily = qs.filter(frequency='D')
        weekly = qs.filter(frequency='W', day_of_week=str(day_of_week))
        monthly = qs.filter(frequency='M', day_of_month=today.day)
        return chain(daily, weekly, monthly)

    digests = SavedSearchDigest.objects.filter(is_active=True,
                                               user__in_reserve=False)
    digests = filter_by_time(digests)
    for obj in digests:
        send_search_digest.s(obj).apply_async()

    not_digest = SavedSearchDigest.objects.filter(is_active=False,
                                                  user__in_reserve=False)
    for item in not_digest:
        saved_searches = item.user.savedsearch_set.filter(is_active=True)
        saved_searches = filter_by_time(saved_searches)
        for search_obj in saved_searches:
            send_search_digest.s(search_obj).apply_async()


@task(name='task.delete_inactive_activations', ignore_result=True)
def delete_inactive_activations():
    """
    Daily task checks if activation keys are expired and deletes them.
    Disabled users are exempt from this check.
    """

    for profile in ActivationProfile.objects.all():
        try:
            if profile.activation_key_expired():
                user = profile.user
                if not user.is_disabled and not user.is_verified:
                    user.delete()
                    profile.delete()
        except User.DoesNotExist:
            profile.delete()


@task(name='tasks.process_user_events', ignore_result=True)
def process_user_events(email):
    """
    Processes all email events for a given user.
    """
    user = User.objects.get_email_owner(email=email)
    logs = EmailLog.objects.filter(email=email,
                                   processed=False).order_by('-received')
    newest_log = logs[0]

    filter_by_event = lambda x: [log for log in logs if log.event in x]

    # The presence of deactivate or stop_sending determines what kind (if any)
    # of My.jobs message the user will receive. deactivate takes precedence.
    # The logs query set has already been evaluated, so the only overhead
    # is the list comprehension
    deactivate = filter_by_event(BAD_EMAIL)
    stop_sending = filter_by_event(STOP_SENDING)
    update_fields = []
    if user and (deactivate or stop_sending) and user.opt_in_myjobs:
        user.opt_in_myjobs = False
        if deactivate:
            user.is_verified = False
            user.deactivate_type = deactivate[0].event
            update_fields.append('is_verified')
            body = """
            <b>Warning</b>: Attempts to send messages to {email} have failed.
            Please check your email address in your <a href="{{settings_url}}">
            account settings</a>.
            """.format(email=deactivate[0].email)
        else:
            user.deactivate_type = stop_sending[0].event
            body = """
            <b>Warning</b>: We have received a request to stop communications
            with {email}. If this was in error, please opt back into emails in
            your <a href="{{settings_url}}">account settings</a>.
            """.format(email=stop_sending[0].email)
        body = body.format(settings_url=reverse_lazy('edit_account'))
        Message.objects.create_message(users=user, subject='', body=body)
        update_fields.extend(['deactivate_type',
                              'opt_in_myjobs'])

    if user and user.last_response < newest_log.received:
        user.last_response = newest_log.received
        update_fields.append('last_response')

    # If update_fields is an empty iterable, the save is aborted
    # This doesn't hit the DB unless a field has changed
    user.save(update_fields=update_fields)
    logs.update(processed=True)


@task(name='tasks.process_batch_events', ignore_result=True)
def process_batch_events():
    """
    Processes all events that have accumulated over the last day, sends emails
    to inactive users, and disables users who have been inactive for a long
    period of time.
    """
    now = date.today()
    EmailLog.objects.filter(received__lte=now-timedelta(days=60),
                            processed=True).delete()

    emails = set(EmailLog.objects.values_list('email', flat=True).filter(
        processed=False))

    result = group(process_user_events.subtask((email, ))
                   for email in emails).apply()
    result.join()

    # These users have not responded in a month. Send them an email if they
    # own any saved searches
    inactive = User.objects.select_related('savedsearch_set')
    inactive = inactive.filter(Q(last_response=now-timedelta(days=30)) |
                               Q(last_response=now-timedelta(days=36)))

    for user in inactive:
        if user.savedsearch_set.exists():
            time = (now - user.last_response).days
            message = render_to_string('myjobs/email_inactive.html',
                                       {'user': user,
                                        'time': time})
            user.email_user('Account Inactivity', message,
                            settings.DEFAULT_FROM_EMAIL)

    # These users have not responded in a month and a week. Stop sending emails.
    User.objects.filter(last_response__lte=now-timedelta(days=37)).update(
        opt_in_myjobs=False)


@task(name="tasks.update_solr_from_model", ignore_result=True)
def update_solr_task(solr_location=None):
    """
    Deletes all items scheduled for deletion, and then adds all items
    scheduled to be added to solr.

    Inputs:
    :solr_location: Dict of separate cores to be updated
    """
    if hasattr(mail, 'outbox'):
        solr_location = settings.TEST_SOLR_INSTANCE
    elif solr_location is None:
        solr_location = settings.SOLR
    objs = Update.objects.filter(delete=True).values_list('uid', flat=True)

    if objs:
        objs = split_list(objs, 1000)
        for obj_list in objs:
            obj_list = filter(None, list(obj_list))
            uid_list = " OR ".join(obj_list)
            for location in solr_location.values():
                solr = pysolr.Solr(location)
                solr.delete(q="uid:(%s)" % uid_list)
        Update.objects.filter(delete=True).delete()

    objs = Update.objects.filter(delete=False)
    updates = []

    for obj in objs:
        content_type, key = obj.uid.split("##")
        model = ContentType.objects.get_for_id(content_type).model_class()
        if model == SavedSearch:
            updates.append(object_to_dict(model, model.objects.get(pk=key)))
        # If the user is being updated, because the user is stored on the
        # SavedSearch document, every SavedSearch belonging to that user
        # also has to be updated.
        elif model == User:
            searches = SavedSearch.objects.filter(user_id=key)
            [updates.append(object_to_dict(SavedSearch, s)) for s in searches]
            updates.append(object_to_dict(model, model.objects.get(pk=key)))
        else:
            updates.append(profileunits_to_dict(key))

    updates = split_list(updates, 1000)
    for location in solr_location.values():
        solr = pysolr.Solr(location)
        for update_subset in updates:
            update_subset = filter(None, list(update_subset))
            solr.add(list(update_subset))
    objs.delete()


def split_list(l, list_len, fill_val=None):
    """
    Splits a list into sublists.

    inputs:
    :l: The list to be split.
    :list_len: The length of the resulting lists.
    :fill_val: The value that is inserted when there are less items in the
        final list than list_len.

    outputs:
    A generator of tuples size list_len.

    """
    args = [iter(l)] * list_len
    return izip_longest(fillvalue=fill_val, *args)


@task(name="tasks.reindex_solr", ignore_result=True)
def task_reindex_solr(solr_location=None):
    """
    Adds all ProfileUnits, Users, and SavedSearches to solr.

    Inputs:
    :solr_location: Dict of separate cores to be updated (Optional);
        defaults to the default instance from settings
    """
    if solr_location is None:
        solr_location = settings.SOLR
    l = []

    u = User.objects.all().values_list('id', flat=True)
    for x in u:
        l.append(profileunits_to_dict(x))

    s = SavedSearch.objects.all()
    for x in s:
        saved_search_dict = object_to_dict(SavedSearch, x)
        saved_search_dict['doc_type'] = 'savedsearch'
        l.append(saved_search_dict)

    u = User.objects.all()
    for x in u:
        l.append(object_to_dict(User, x))

    l = split_list(l, 1000)

    for location in solr_location.values():
        solr = pysolr.Solr(location)
        for x in l:
            x = filter(None, list(x))
            solr.add(x)


def parse_log(logs, solr_location):
    """
    Turns a list of boto keys into a list of dicts, with each dict representing
    a line from the keys

    Inputs:
    :logs: List of logs generated by boto that reference files on s3
        Lines in analytics logs are formatted as follows:
            %{%Y-%m-%d %H:%M:%S}t %a %m %U %q %H %s %{Referer}i %{aguid}C
                %{myguid}C %{user-agent}i
        Lines in redirect logs are formatted slightly differently:
            %{%Y-%m-%d %H:%M:%S}t %a %m %U %{X-REDIRECT}o %p %u %{X-Real-IP}i
                %H "%{User-agent}i" %{r.my.jobs}C %{Referer}i %V %>s %O %I %D

    :solr_location: Dict of separate cores to be updated (Optional);
        defaults to the default instance from settings
    """
    # Logs are potentially very large. If we are going to look up the company
    # associated with each hit, we should memoize the ids.
    log_memo = {}

    for log in logs:
        to_solr = []
        path = '/tmp/parse_log'
        # Ensure local temp storage for log files exists
        try:
            os.mkdir(path)
        except OSError:
            if not os.path.isdir(path):
                raise
        f = open('%s/%s' % (path, uuid.uuid4().hex), 'w+')
        try:
            log.get_contents_to_file(f)
            f.seek(0)

            for line in f:
                if line[0] == '#':
                    # Logs contain a header that LogParser uses to determine
                    # the log format; if we see this, ignore it
                    continue

                # line in f does not strip newlines if they exist
                line = line.rstrip('\n')
                line = line.split(' ')

                # reconstruct date and time
                line[0] = '%s %s' % (line[0], line[1])
                # turn date and time into a datetime object
                line[0] = datetime.strptime(line[0], '%Y-%m-%d %H:%M:%S')
                # remove the time portion, which is now merged with the date
                del line[1]

                # reconstruct user agent
                # and remove it from the line
                if 'redirect' in log.key:
                    ua = ' '.join(line[9:-7])
                    del line[9:-7]
                else:
                    ua = line[8]
                    del line[8]

                if not helpers.is_bot(ua):
                    # Only track hits that come from actual users
                    update_dict = {
                        'view_date': line[0],
                        'doc_type': 'analytics',
                    }

                    # Make sure the value for a given key is only a list if
                    # there are multiple elements
                    qs = dict((k, v if len(v) > 1 else v[0])
                              for k, v in urlparse.parse_qs(
                                  line[4]).iteritems())

                    if 'redirect' in log.key:
                        aguid = qs.get('jcnlx.aguid', '')
                        myguid = qs.get('jcnlx.myguid', '')
                        update_dict['view_source'] = qs.get('jcnlx.vsid', 0)
                        update_dict['job_view_buid'] = qs.get('jcnlx.buid', '0')

                        # GUID is the path portion of this line, which starts
                        # with a '/'; Remove it
                        update_dict['job_view_guid'] = line[3][1:]
                        update_dict['page_category'] = 'redirect'
                        domain = qs.get('jcnlx.ref', '')
                        domain = urlparse.urlparse(domain).netloc
                        update_dict['domain'] = domain
                    else:
                        aguid = qs.get('aguid', '')
                        myguid = qs.get('myguid', '')
                        update_dict['view_source'] = qs.get('jvs', 0)
                        update_dict['job_view_buid'] = qs.get('jvb', '0')
                        update_dict['job_view_guid'] = qs.get('jvg', '')
                        update_dict['page_category'] = qs.get('pc', '')

                        # These fields are only set in analytics logs
                        update_dict['domain'] = qs.get('d', '')
                        update_dict['facets'] = qs.get('f', '')
                        update_dict['job_view_title_exact'] = qs.get('jvt', '')
                        update_dict['job_view_company_exact'] = qs.get('jvc', '')
                        update_dict['job_view_location_exact'] = qs.get('jvl', '')
                        update_dict['job_view_canonical_domain'] = qs.get('jvcd', '')
                        update_dict['search_location'] = qs.get('sl', '')
                        update_dict['search_query'] = qs.get('sq', '')
                        update_dict['site_tag'] = qs.get('st', '')
                        update_dict['special_commitment'] = qs.get('sc', '')

                    # Handle logs containing the old aguid/myguid formats
                    aguid = aguid.replace('{', '').replace('}', '').replace('-', '')
                    update_dict['aguid'] = aguid

                    myguid = myguid.replace('-', '')

                    if myguid:
                        try:
                            user = User.objects.get(user_guid=myguid)
                        except User.DoesNotExist:
                            update_dict['User_user_guid'] = ''
                        else:
                            update_dict.update(object_to_dict(User, user))

                    buid = update_dict['job_view_buid']
                    domain = update_dict.get('domain', None)
                    if not (buid in log_memo or domain in log_memo):
                        # We haven't seen this buid or domain before
                        if buid == '0' and domain is not None:
                            # Retrieve company id via domain
                            try:
                                site = SeoSite.objects.get(domain=domain)
                                company_id = site.business_units.values_list(
                                    'company__pk', flat=True)[0]
                            except (SeoSite.DoesNotExist,
                                    IndexError):
                                # SeoSite.DoesNotExist: Site does not exist
                                #   with the given domain
                                # IndexError: SeoSite exists, but is not
                                #   associated with business units or companies
                                company_id = 999999
                            key = domain
                        else:
                            # Retrieve company id via buid
                            try:
                                # See if there is a company associated with it
                                company_id = Company.objects.filter(
                                    job_source_ids=buid)[0].pk
                            except IndexError:
                                # There is not; default to DirectEmployers
                                # Association
                                company_id = 999999
                            key = buid

                        # The defining feature of a given document will either
                        # be the domain or the buid.
                        # Our memoization dict will have the following structure
                        # {str(buid): int(company_id),
                        #  str(domain): int(company_id)}
                        log_memo[key] = company_id

                    # By this point, we are guaranteed that the correct key is
                    # in log_memo; pull the company id from the memo dict.
                    if domain is not None and domain in log_memo:
                        update_dict['company_id'] = log_memo[domain]
                    else:
                        update_dict['company_id'] = log_memo[buid]

                    update_dict['uid'] = 'analytics##%s#%s' % \
                                         (update_dict['view_date'], aguid)
                    to_solr.append(update_dict)
        except Exception:
            # There may be more logs to process, don't propagate the exception
            pass
        finally:
            # remove the file from the filesystem to ensure we don't fill the
            # drive (again)
            f.close()
            os.remove(f.name)

        # Ensure all hits get recorded by breaking a potentially massive list
        # down into something that solr can manage
        subsets = split_list(to_solr, 500)
        for location in solr_location.values():
            solr = pysolr.Solr(location)
            for subset in subsets:
                try:
                    subset = filter(None, subset)
                    solr.add(subset)
                except pysolr.SolrError:
                    # There is something wrong with this chunk of data. It's
                    # better to lose 500 documents than the entire file
                    pass


@task(name="tasks.delete_old_analytics_docs", ignore_result=True)
def delete_old_analytics_docs():
    """
    Deletes all analytics docs from the "current" collection that are older
    than 30 days
    """
    if hasattr(mail, 'outbox'):
        solr_location = settings.TEST_SOLR_INSTANCE['current']
    else:
        solr_location = settings.SOLR['current']

    pysolr.Solr(solr_location).delete(
        q="doc_type:analytics AND view_date:[* TO NOW/DAY-30DAYS]")


@task(name="tasks.update_solr_from_log", ignore_result=True)
def read_new_logs(solr_location=None):
    """
    Reads new logs and stores their contents in solr

    Inputs:
    :solr_location: Dict of separate cores to be updated (Optional);
        defaults to the default instance from settings
    """
    # If running tests, use test instance of local solr
    if hasattr(mail, 'outbox'):
        solr_location = settings.TEST_SOLR_INSTANCE
    elif solr_location is None:
        solr_location = settings.SOLR

    conn = boto.connect_s3(aws_access_key_id=settings.AWS_ACCESS_KEY_ID,
                           aws_secret_access_key=settings.AWS_SECRET_KEY)
    # https://github.com/boto/boto/issues/2078
    # validate=True costs 13.5x validate=False; Skip validation if we are
    # reasonably certain that the bucket exists.
    log_bucket = conn.get_bucket('my-jobs-logs', validate=False)

    # Sort logs into groups based on server
    all_logs = log_bucket.list()
    logs_by_host = {}
    for log in all_logs:
        # Logs are being stored with a key of host/log_type/file.log
        key_parts = log.key.split('/')

        # Since we are also storing redirect error logs, we should ensure
        # we are only processing the logs we care about
        if key_parts[1] in ['analytics', 'redirect']:
            logs_by_host.setdefault(key_parts[0], []).append(log)

    logs_to_process = []
    for key in logs_by_host.keys():
        # Files are named by date and time; the last log for each host is the
        # last log uploaded by that host
        logs_to_process += logs_by_host[key][-1:]

    # Ensure we only process each['href'] file once
    processed = getattr(settings, 'PROCESSED_LOGS', set())
    unprocessed = [log for log in logs_to_process if log.key not in processed]

    parse_log(unprocessed, solr_location)

    settings.PROCESSED_LOGS = set([log.key for log in logs_to_process])

    delete_old_analytics_docs.delay()


@task(name='tasks.expire_jobs', ignore_result=True)
def expire_jobs():
    jobs = Job.objects.filter(date_expired=date.today(),
                              is_expired=False, autorenew=False)
    for job in jobs:
        # Setting is_expired to True will trigger job.remove_from_solr()
        job.is_expired = True
        job.save()

    jobs = Job.objects.filter(date_expired=date.today(),
                              is_expired=False, autorenew=True)
    for job in jobs:
        job.date_expired = date.today() + timedelta(days=30)
        # Saving will trigger job.add_to_solr().
        job.save()


@task(name="tasks.task_update_solr", acks_late=True, ignore_result=True)
def task_update_solr(jsid, **kwargs):
    try:
        import_jobs.update_solr(jsid, **kwargs)
    except:
        logging.error(traceback.format_exc(sys.exc_info()))
        raise task_update_solr.retry()


@task(name='tasks.etl_to_solr', ignore_result=True)
def task_etl_to_solr(guid, buid, name):
    try:
        import_jobs.update_job_source(guid, buid, name)
    except Exception as e:
        logging.error("Error loading jobs for jobsource: %s", guid)
        logging.exception(e)
        raise task_etl_to_solr.retry()


@task(name="tasks.task_clear_solr", ignore_result=True)
def task_clear_solr(jsid):
    """Delete all jobs for a given Business Unit/Job Source."""
    import_jobs.clear_solr(jsid)


@task(name="tasks.task_force_create", ignore_result=True)
def task_force_create(jsid):
    import_jobs.force_create_jobs(jsid.id)


@task(name="tasks.task_submit_sitemap", ignore_result=True)
def task_submit_sitemap(domain):
    """
    Submits yesterday's sitemap to google for the given domain
    Input:
        :domain: sitemap domain
    """
    ping_google('http://{d}/sitemap.xml'.format(d=domain))


@task(name="tasks.task_submit_all_sitemaps", ignore_result=True)
def task_submit_all_sitemaps():
    sites = SeoSite.objects.all()
    for site in sites:
        task_submit_sitemap.delay(site.domain)
